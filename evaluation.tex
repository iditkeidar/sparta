\section{Evaluation} \label{sec:eval}

We compare the performance of \alg\ to the following algorithms: parallel BMW (\pBMW)~\cite{rojas2013efficient}, 
the best-in-class multiprocessor implementation we are aware of, a parallel implementation of RA (\pRA), 
and a na\"{\i}ve parallel implementation of NRA (\pNRA). Although our primary focus is on the approximate 
variants of all algorithms, for completeness, we also examine the performance of their exact counterparts.
%
% We study all these algorithms in both exact and approximate variants -- the latter over a spectrum of heuristic parameters that strike different performance-vs-recall tradeoffs.  

In what follows, Section~\ref{ssec:setup} describes the experiment setup, 
Section~\ref{ssec:implementation} explains how the algorithms are 
implemented, and Section~\ref{ssec:results} presents our results.

\subsection{Experiment Setup}
\label{ssec:setup}

We study the algorithms in terms of query latency and throughput attainable at a single multi-core server. 
We use mid-tier industry-standard hardware -- a 12-core Intel Xeon E5620 with 24GB RAM and 1TB SSD drive. 

\subsubsection{Evaluation Framework}

The benchmarking environment and the algorithms  are implemented in Java. 
A  \emph{benchmark driver} draws queries from an input queue and submits them to the algorithm being tested. 
It runs the queries either one-by-one (latency mode) or in parallel (throughput mode).
In both modes, the algorithm exploits a thread pool for intra-query parallelism. 
The driver controls the pool size by notifying the algorithm how many threads are available to it.
In the throughput evaluation mode, the pool is shared by multiple queries; 
new queries are scheduled for execution once  there are idle threads with no outstanding work from 
the currently executing queries.


In all experiments, the index is pre-built offline and stored on disk as a collection of binary files, 
each storing a shard of data partitioned by term. The benchmark environment memory-maps the content 
of these files via the MappedByteBuffer API\footnote{\small{\url{https://docs.oracle.com/javase/7/docs/api/java/nio/MappedByteBuffer.html}}}. 
%such that the algorithm code can access the data identically to RAM. 
The application code makes no attempt to optimize the disk access beyond the standard filesystem mechanisms (e.g., caching 
of popular blocks). 

\subsubsection{Datasets}
We  experiments with two document corpora. The first  is the TREC {\cw\/} dataset, 
Category B ({\cw}09B)~\cite{ClueWeb09}, which is widely used for information retrieval research. This dataset includes approximately 50M web documents. 
The second corpus is a synthetic 10x scale-up of \cw, named \cwten, which we created to explore the algorithms' scalability 
with the dataset size. 

\cwten\ is a superset of {\cw}09B.
The 450M synthetic documents in {\cwten\/} are generated as follows. Each document is a bag of words drawn from the original {\cw\/} dictionary 
(the order is immaterial for our document scoring function, see Section~\ref{ssec:scoring}) so that the number of occurrences of a term $t_i$ with an original 
global frequency rate of $F(t_i)$ is drawn from a geometric distribution with a stopping probability of $1-F(t_i)$. This document generation process preserves 
the  term frequency distribution of {\cw\/} in \cwten.
 
We use the popular Lucene open-source search engine\footnote{\small{\url{https://lucene.apache.org/}}} for text tokenization, posting list maintenance, 
and term statistics retrieval.

We submit queries based on the public AOL search log\footnote{\small{\url{http://www.cim.mcgill.ca/~dudek/206/Logs/AOL-user-ct-collection/}}}.
For each number of terms from $1$ to $12$, we independently sample $100$ queries of this length uniformly at random from the AOL log.
We also separately experimented with a query log of another commercial web search engine; this experiment  
produced statistically similar results, and so we omit them here.  

\subsubsection{Document Scoring}
\label{ssec:scoring}
For document evaluation, we apply a simple tf-idf score function with document length normalization~\cite{Baeza-Yates:1999:MIR:553876} as follows:
\[ \textit{ts}(D, t_i) \triangleq \frac{\textit{idf}(t_i) \cdot \textit{freq}(D, t_i)}{\sqrt{|D|}},\]
where \textit{freq}$(D, t_i)$ is the frequency of term $t_i$ in document $D$,
$\textit{idf}(t_i)$ is  the inverse document frequency of term $t_i$,
and $|D|$ is the number of tokens in $D$. 

\subsection{Implementation}
\label{ssec:implementation}

All algorithms store posting lists in the index as contiguous uncompressed arrays. In addition, {\pRA} stores 
its secondary index (document id to position in the posting list mapping) in the same form. 
Term scores are stored in the posting lists as integers, after scaling (with a factor of $10^6$) and rounding. 
Using integer arithmetics instead of floating-point significantly speeds up document evaluation. 

The specific algorithm implementations are as follows.

\subsubsection{\pBMW}
Our implementation of {\pBMW} closely follows the description in~\cite{rojas2013distributing}. The algorithm partitions the execution of the 
sequential BMW~\cite{Ding:2011} among multiple threads. Each thread handles a distinct subset of documents, and computes a local top-k 
result. The algorithm then merges the partial results to obtain the final top-k. 
%See Section~\ref{sec:related} for detail.

Similarly to \alg, \pBMW's threads obtain jobs from a common job queue. Here, a job defines a range of document ids to scan. 
We set the number of jobs to be twice the number of worker threads, and assign equal-size ranges to all threads.  
This partition results in well-balanced execution in which the whole worker pool is utilized 
most of the time. 

Each thread maintains a thread-local heap with the current top-k documents. (We also experimented with a shared heap and 
got inferior results; a similar finding was reported in~\cite{rojas2013distributing}.)
Similarly, each thread $T$ maintains a local threshold $\Theta_T$ for filtering heap insertions; 
$\Theta_T$ is \emph{at least} the lowest score in the local heap, but may be higher thanks to the progress of other threads.  
Specifically, threads benefit from each other's progress via a shared $\Theta$ variable. 
Thread $T$ periodically compares $\Theta$ to its local $\Theta_T$, and promotes the smaller of the two to $\max(\Theta_T, \Theta)$. 
This way,  slower workers catch up with  faster ones.

\pBMW, which is applied to a segment of posting lists, further splits the segment into blocks, and uses block-level
statistics to prune the search~\cite{Ding:2011}. We experimented with multiple block sizes and selected $64$, 
which yielded the best performance.

In the approximate version of  \pBMW\
pruning aggressiveness is  controlled by  a parameter 
$f \geq 1$, 
a constant that multiplies $\Theta$ to obtain a higher threshold for document score upper bounds~\cite{Broder:2003}. For $f=1$, the algorithm is exact.


\subsubsection{\pRA\ and \pNRA}

The {\pNRA} algorithm is a na\"{\i}ve parallelization of NRA that does not employ \alg's optimizations. 
Namely, it uses a shared document map, which it does not clean, and updates the term
upper bounds upon every document evaluation. As in \alg, a dedicated task checks the stopping condition.
(We experimented also with distributed stopping detection and got worse results).

Similarly to {\pNRA} and \alg, {\pRA} traverses the posting lists in segments, using a job queue. 
In contrast with them, {\pRA} computes complete document scores using its secondary index, 
instead of maintaining partial scores. 
Note that {\pRA} is the only studied algorithm that exercises random access to persistent storage.  

{\pRA} maintains its results in a shared heap (experiments did not show any advantage to using local heaps).
Note that the algorithm's multiple worker threads may encounter postings of the same document independently, 
and consequently score that document and try to insert it into the heap multiple times. The implementation guarantees 
the uniqueness of insertion (only the first one takes effect).

%{\pRA} halts execution if no new results have been produced for $\Delta$ time ($\Delta=\infty$ means the result is exact). 
Since in this case, stopping detection is lightweight, we do not dedicate a task to it. Instead, all workers check the  
\RAStop\ condition and also monitor the time elapsed since the last heap update.  A thread that detects that the algorithm 
can stop notifies all threads.


\subsection{Results}
\label{ssec:results}

We evaluate the query processing latency and throughput induced by the candidate algorithms, for $k=1000$.
(Experiments with $k=100$ produced qualitatively similar results).
 
We instantiate three incarnations of each algorithm A: exact (denoted: A\ex), high-recall (denoted: A\hi), and
low-recall (denoted: A\lo). Note that the approximate algorithms' parameters ($\Delta$ and $f$) affect the recall
but do not directly control it; our high recall instances are ones that empirically achieve a  recall of $97\%$ or 
higher for both \cw\/ and \cwten. 

\subsubsection{Exact Algorithms}

\begin{table}[tbp]
\begin{center}
\begin{tabular}{| c | c | c | c | c | }
\hline
  & Sparta & \pNRA & \pRA & \pBMW \\ \hline
 ClueWeb & 860 & 13\!,291 & 480 & 750 \\ \hline
 ClueWebX10 & 12\!,010 & $90\!,000+$ & $7\!,410$ & $10\!,210$ \\
\hline
\end{tabular}
\end{center}
\caption{Average query latency (in ms) of 12-term queries for the exact algorithms using 12 worker threads. 
None of the algorithms scales to meet expected real-time SLAs. }
\label{tab:safe-latency}
\end{table}

Our first experiment shows that none of the exact algorithms meets a real-time SLA for for verbose queries. 
Table~\ref{tab:safe-latency} depicts the mean processing latencies of 12-term queries with 12 worker 
threads (i.e., a single query fully exploits the multi-core CPU). Most of the algorithms complete within 
500 ms to 1 second for when applied to the {\cw} dataset, and run for many seconds when applied to \cwten. 
{\pRA} emerges as the fastest algorithm in this setting. We posit that this is tightly linked to its theoretical 
instance-optimality property. We see that \pNRA\/ is much worse than the other algorithms. Its average latency exceeds 
13 seconds on \cw\/ and 1.5 minutes on \cwten.

We will revisit the algorithms' execution dynamics -- namely, how fast they accrue their results -- 
as we study approximate algorithms in the sequel. 
 
\subsubsection{Approximate Algorithms}
 
With  exact instances of all  algorithms failing to match  real-time requirements, we turn to focus on the approximate instances. 
Namely, we parameterize \alg, \pRA\/ and \pNRA\/ with $\Delta=10$ ms for high recall and $\Delta=5$ ms for low recall. (The low-recall 
algorithms' performance presentation is omitted from the sequel because most high-recall versions match the SLA). \pBMW\/ is instantiated 
with $f=5$ for high recall and $f=10$ for low recall.  

{\bf Accuracy.\ } Table~\ref{tab:recall-mrr-distance} depicts the empirical accuracy results for 12-term queries. We see that the MRR-distance metric 
is closely correlated with recall -- e.g., \alg\hi\/ yields $99\%$ recall and $0.002$ MRR-distance when applied to \cwten. That is, the approximate 
algorithms not only  retain a large fraction of the true results, but also succeeds in retaining the highest ranked ones. 

\remove{
This result is particularly interesting for \alg, which only computes partial scores. In practice, these scores are very close to the complete ones. 
In our experiments, on average, $93\%$ of the top-k documents have a correct score at the end of the execution. The average Pearson correlation
 between the correct scores and the partial scores is $0.999$.%, whereas the average Kendall's $\tau$ rank correlation is $0.86$. 
}

Summing up, under certain parameter choices the approximate algorithms achieve very high accuracy. We now focus on their performance. 
  
\begin{table*}[htbp]
\centering
\begin{tabular}{| c | c | c | c | c | c |}
\hline
  & \alg\hi &  \pRA\hi & \pNRA\hi & \pBMW\hi & \pBMW\lo \\ \hline
  \cw & 97.5\% / 0.004 &  98.5\% / 0.002 & 98.5\% / 0.002 & 97.5\% / 0.009 & 80\% / 0.048 \\ \hline
  \cwten & 99\% / 0.002 & 99\% / 0.001 & 99\% / 0.001 & 97\% / 0.009 & 79.9\% / 0.099 \\
\hline
\end{tabular}
\caption{Accuracy metrics (Recall / MRR-distance) induced by the approximate algorithms for 12-term queries.}
\label{tab:recall-mrr-distance}
\end{table*}

{\bf Latency.\ } 
Figure~\ref{fig:terms-scaling} depicts the scaling of single-query latency of the approximate algorithms under study, 
as the number of terms scales from $1$ to $12$. In each experiment, the number of workers allocated to the algorithm 
is equal to the number of terms (for \alg, \pRA, and \pNRA, this means maximal parallelism). 

Figure~\ref{fig:terms-scaling}(a) and Figure~\ref{fig:terms-scaling}(b) present, respectively, the mean latency and the 
$95$-th percentile latency of queries applied to the {\cw} dataset. The latter captures the so-called ``tail latency'' of 
the slowest $5\%$ of the queries;  Figure~\ref{fig:terms-scaling}(c) depicts the mean latency
for \cwten.

\begin{figure*}[tbh]
\centering
\begin{tabular}{ccc}
      \begin{subfigure}[t]{0.33\textwidth}
         \includegraphics[width=\textwidth]{figures/latency_12threads_clueweb.pdf}
        \caption[]{Average latency, \cw}
      \end{subfigure}     

	\begin{subfigure}[t]{0.33\textwidth}
    	\includegraphics[width=\textwidth]{figures/latency_95th_percentile_clueweb.pdf}
	\caption{95\% latency, \cw}
    \end{subfigure}  

    \begin{subfigure}[t]{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/latency_12threads_cluewebX10.pdf}
	\caption{Average latency, \cwten}
    \end{subfigure}  
\end{tabular}
\caption{Top-k ($k=1000$) query latency scaling with the number of query terms, for different instances of \alg, \pRA, and \pBMW. 
The intra-query parallelism is equal to the number of terms in all algorithms. }
\label{fig:terms-scaling}
\end{figure*}

\commentOut{
\begin{figure}[tbh]
\centering
\begin{tabular}{ccc}
      \begin{subfigure}[t]{0.33\textwidth}
         \includegraphics[width=\textwidth]{figures/throughput_12threads_clueweb.pdf}
      \end{subfigure}      
\end{tabular}
\caption{Top-k query throughput scaling with the number of query terms. The intra-query parallelism 
is equal to the number of terms in all parallel algorithms.}
\label{fig:throughput-scaling}
\end{figure}
}

\alg\/ outperforms its competitors throughout all query sizes. The margin is especially big for verbose queries
($5+$ terms). \alg's average query latency scales perfectly with the dataset size, 
remaining below $180$ ms for all query lengths for both \cw\/ and \cwten. In other words, \alg's result set solidifies after processing 
a similar number of postings, even when the overall index size grows $10$x!  The $95\%$ latency is below $500$ ms. 

\pRA\/ exhibits much weaker scalability. For example, for $12$-term queries it is slower by more than $2$x 
when applied to \cw, and by more than $10$x (!) when applied to \cwten. We explain this by the cost of evaluating the complete 
document scores in \pRA, which forces extremely intensive access to the secondary index. This translates to volumes of random 
I/O that cannot be sustained even with modern SSD hardware. Note that this trend is the reverse of the one observed for \pRA\ex, 
which outperforms \alg\ex\/ (Table~\ref{tab:safe-latency}). That is, \alg\/ spends much more work than \pRA\/ 
in order to collect the remaining $2.5\%$ of the exact result set. We revisit this phenomenon  below. 

As an aside, note that \pRA\/ scales somewhat better if its whole working set fits into memory. 
However, this is seldom the case when diverse queries are applied to a large index.
%, e.g., when the same small set of queries is evaluated over and over again. 
%This use case is of limited interest.  
%besides, it does not manifest with the \cwten\/ dataset in statistically significant way. 

The unoptimized \pNRA\/ is the weakest option when applied to \cw\/ ($1$s average latency for 12-term queries)
but emerges as the second-best on \cwten\/ (latency only grows to $1.12$s). This is thanks to the high scalability 
of the approximate NRA approach.
Still, it is much slower than
\alg. These results emphasize the importance of the locality optimizations employed in \alg's design (especially the background 
cleaning of \DMap), which substantially improve the hardware cache performance. We omit \pNRA\/ from 
further discussion. 

\pBMW, the best-in-class algorithm in the literature, fails to match \alg's speed. For example, for 12-term queries, 
\pBMW\hi\/ completes  within $630$ ms on average on \cw, and within as long as $9.9$ seconds on \cwten. \pBMW\lo, 
which sacrifices $20\%$ of the recall for performance, only succeeds to improve these latencies by $10\%$ to $15\%$. 

\begin{figure*}[hbt]
\centering
\begin{tabular}{ccc}
      \begin{subfigure}[t]{0.33\textwidth}
         \includegraphics[width=\textwidth]{figures/cumulative_12threads_clueweb.pdf}
        \caption[]{ClueWeb}
        \label{fig:dynamics-clueweb}
      \end{subfigure} 
    
& 
	\hspace{0.1\textwidth}
& 
      \begin{subfigure}[t]{0.33\textwidth}
      	\includegraphics[width=\textwidth]{figures/cumulative_12threads_cluewebX10.pdf}
	    \caption{ClueWebX10}
	\label{fig:dynamics-cluewebX10}
      \end{subfigure}  

\end{tabular}
       \caption{Recall dynamics with elapsed time, for 12-term queries, with 12 worker threads.}
       \label{fig:dynamics}
\end{figure*}

{\bf Recall dynamics.\ } 
In order to understand how the top-k results get accrued by the different algorithms, we zoom in on the dynamics of query 
recall over the running time. We focus on 12-term queries in a 12-worker configuration. 
Figure~\ref{fig:dynamics}(a) and Figure~\ref{fig:dynamics}(b) present the results for the \cw\/ and \cwten\/ datasets, respectively. 
Because the approximate versions of \alg\ and \pRA\ are  identical to the respective exact versions until they stop, 
we show the dynamics of the exact versions only.  The same is not true for \pBMW, where $f$ impacts the algorithm's results from the outset.
Hence, we show the dynamics of all three instances of \pBMW. 
In the exact algorithms's curves, the rightmost data point corresponds to the exact algorithm's completion time.   

We see that \alg's recall growth is the fastest. For instance, it surpasses $80\%$ recall (the \alg\lo\/ instance) in less than $50$ ms, 
and $90\%$ recall in less than 100. But over time, its returns  diminish, and most of the work becomes non-productive. Whereas
\pRA\/ takes much longer to converge because it needs to fully score each encountered document,  its concluding phase is more efficient because 
most relevant documents   have complete  scores. 
The \pBMW\/ variants, on the other hand, scan the postings in the order of document ids, which is unrelated to document scores, and hence accumulate the true hits 
at a near-linear rate. Obviously, the convergence rate of \pBMW\hi\/ and \pBMW\lo\/ is faster than that of \pBMW\ex. For both datasets, the first 
two accrue results at similar rates until \pBMW\lo\/ stops at approximately $80\%$ recall. 

%Figure~\ref{fig:terms-scaling}(c) zooms in on \alg's performance versus other variants of NRA: AllPar -- a na\"{\i}ve parallelization  (without the initial sequential phase),  Seq -- a sequential implementation, and SharedState -- a version of \alg\ that does not use local \TMap\/s. Neither of the first two scales with real-time latency. Interestingly, AllPar is slower than Seq, underscoring the importance of aggressive pruning prior to starting the concurrent execution. SharedState is substantially faster, but fails to utilize the hardware cache efficiently due to lack of locality. It thus lags behind \alg\/ by approximately $35\%$ for long queries. 
{\bf Parallelism.\ } 
We next study the scaling of query latency with  intra-query parallelism. We consider  $12$-term queries with a number of threads varying from $1$ to $12$. 
The results appear in Figure~\ref{fig:threads-scaling}(a) (for \cw) and Figure~\ref{fig:threads-scaling}(b) (for \cwten). The latter depicts only \alg\/ and \pRA\/ 
because none of the \pBMW\/ instances scales close to real-time performance.  \alg\ requires some level of parallelism for real-time speed -- e.g., for \cw\/ the sequential
latency is $840$ ms, which is  above typical SLA requirements. Most of the gain is achieved at low-parallelism levels (2 threads suffice). 
On the other hand, for \pBMW, much higher parallelism is essential -- its latency is inversely proportional to the number of threads. Thus, \alg\ is not only faster 
than \pBMW, but also requires less resources, which benefits throughput as we next show.
%offers the system designer a latency-throughput tradeoff if some slack in latency can be tolerated. 

\begin{figure*}[tbh]
\centering
\begin{tabular}{ccc}
      \begin{subfigure}[t]{0.33\textwidth}
         \includegraphics[width=\textwidth]{figures/latency_12terms_clueweb.pdf}
        \caption[]{ClueWeb}
    %    \label{fig:varying-threads-6-terms}
      \end{subfigure} 
& 
	\hspace{0.1\textwidth}
& 
      \begin{subfigure}[t]{0.33\textwidth}
      \includegraphics[width=\textwidth]{figures/latency_12terms_cluewebX10.pdf}
	  \caption{ClueWebX10}
	% \label{fig:varying-threads-12-terms}
      \end{subfigure}
\end{tabular}
\caption{Top-k query latency scaling with intra-query parallelism, for $12$-term queries.}
\label{fig:threads-scaling}
\end{figure*}

\begin{table}[htb]
\centering
\begin{tabular}{| c | c  | c | c | c | }
\hline
  & \alg &  \pRA & \pBMW & \pBMW \\ 
  & \hi &  \hi & \hi & \lo \\ \hline
  \cw &  12.5 &  10.9 & 5.95 &  6.64 \\ \hline
  \cwten & 9.6 & 1.8 & 0.38 & 0.42 \\
\hline
\end{tabular}
\caption{Average throughput (in queries per second) of the approximate algorithms on a query distribution measured for voice queries in production. }
\label{tab:thpt}
\end{table}

{\bf Throughput.\ } Finally, we compare the throughput (in queries per second) provided by the different
algorithms. To this end, we generate a workload with the query size distribution reported in~\cite{sigir/Guy16},
where the average query length is $4.2$ (std: $2.96$), and more than $5\%$ of the queries are $10$ terms or longer.
The queries are generated as follows: we first sample a query length $\ell$ from the distribution in~\cite{sigir/Guy16}, and then 
select a query uniformly at random among all the length-$\ell$ queries in the complete set of  $1200$ AOL queries. 

Table~\ref{tab:thpt} depicts the results of running this query mix on a shared worker pool  of $12$ threads. 
Here too, \alg\/ improves over its competitors by a wide margin, especially on \cwten, where its throughput
is 25x that of \pBMW\hi. This  advantage is thanks to a combination of \alg's speed and lower resource utilization.

 