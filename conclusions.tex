\section{Conclusions}
\label{sec:conclusions}

We presented \alg\/ -- a practical parallel algorithm that exploits multi-core hardware for approximate top-k retrieval 
within interactive latency bounds. While being fast in general, it is tailored for very long queries ($10$ or more terms), which  
are becoming prevalent in modern conversational products.  It is also extremely scalable with the corpus size.

\alg\ leverages the efficiency and early-stopping properties of the Threshold Algorithm; it forgoes the need for random access
and duplicate indices by using the ``lazy'' scoring approach of TA's NRA variant.  It then optimizes memory footprints, memory access 
patterns, inter-thread data sharing, and synchronization in order  to obtain high performance on  multi-core hardware.

We showed that \alg\/ yields sub-$180$ ms average latencies on standard hardware
for multiple query categories ($1$ to $12$ terms) when applied to datasets of up to $500$M documents. The algorithm 
produces a highly accurate approximation of the exact results (a recall of above $97.5\%$ and an MRR-distance of below
$0.004$). A state-of-the-art parallel algorithm (\pBMW) providing similar accuracy required $640$ ms on a $50$M-document dataset, 
and  $9.9$ seconds on a $500$M-document corpus in our experiments. \alg\/ also achieved 25x higher throughput than \pBMW\ 
on the large corpus, for a query mix with the 
length distribution measured for voice queries in production. 

