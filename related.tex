\section{Related Work}
\label{sec:related}

Verbose queries challenge standard top-k processing techniques in terms of runtime latency. Huston and Croft \cite{Huston:2010} evaluated several sequential query processing techniques for verbose queries, concluding that the most effective one is to simply reduce the length of the query by omitting stop-words or ``stop-structure'' expressions. 
In this work we ignore the query pre-processing phase and consider the query as a bag of words given after textual analysis.

Crane et al.~\cite{Crane:2017} showed that document-at-a-time processing algorithms are susceptible to tail queries that may take orders of magnitude longer than the median query. Additionally, approximate query evaluation in WAND and BMW does not significantly reduce the variance. Moreover, in agreement with our findings, they showed that score-at-a-time algorithms, which access the  posting lists in decreasing impact order, are less sensitive to tail queries due to their effective early termination capability. 

With the rapid development in multiprocessor hardware, a new line of research has emerged on exploiting multi-threaded programing for top-k retrieval. Tatikonda 
et al.~\cite{Tatikonda:2011} studied parallel top-k retrieval for web queries that are processed in conjunctive mode. They improved the performance of posting list intersection by considering various coarse-grained and fine-grained parallelization models. The work associated with a given query was partitioned into a number of small and independent tasks that were subsequently processed in parallel.  Similarly, Bonacic et al.~\cite{Bonacic:2010} parallelized the query processing by dividing the document space into chunks; each chunk of the inverted file was assigned to a thread, which identified the (local) top-k results of that portion using WAND. Finally, a broker aggregated the top results of all threads for extracting the final output. 

Rojas et al.~\cite{rojas2013efficient} proposed a multi-threaded implementation for BMW. Similarly to \cite{Bonacic:2010}, each thread applies BMW over a portion of the inverted file in order to identify the (local) top-k results. In one implementation, each thread maintains a local heap. Alternatively, a global heap is shared by all threads, and so no final merge is needed. More importantly, the global threshold, which is the current minimum document score in the heap, is available to all threads. The global threshold is much tighter than the threads' local thresholds, hence much less work is done by each of the threads as  more documents can be safely skipped. However, additional locks are needed to guarantee exclusive updates of the shared heap. The experimental results described in \cite{rojas2013efficient} show the superiority of the local-heap over the global-heap version. The pBMW implementation used in our experiments follows the local-heap version described above,
but periodically shares the  $\Theta$ values among the threads for improved performance.

Jeon et al.~\cite{Jeon:2014} argued that parallelizing the processing of an individual query gives limited benefits compared to sequential execution since short-running queries, which dominate the workload, do not benefit from parallelization. On the other hand, parallelization substantially reduces the execution time of long-running queries.  They presented an adaptive resource management algorithm that chooses the degree of parallelism at runtime for each query, based on predicting high-latency queries.

%DC not related:
Ao et al.~\cite{Ao:2011} explored using GPU hardware for information retrieval. They focused on optimizing throughput rather than latency. 

The Threshold Algorithm and its variants \cite{Fagin:2001,Fagin:2003,Akbarinia:2007} have been extensively studied by the database community, and have been applied in many relational database systems (for a comprehensive survey see \cite{ilyas2008survey}). 
Mamoulis et al.~\cite{Mamoulis:2007} observed two main phases during NRA processing -- the ``growing phase'', where the candidate list grows, and the ``shrinking phase'' where no new documents can end up in the top-k results, after the first stopping condition is met. They used different data structures for the two phases in order to minimize the number of accesses and the memory requirements. 
Gursky et al.~\cite{Gursky:2008} also noticed the bottleneck in NRA computation derived from NRA's needs to maintain an extremely large number of partially scored candidates. They proposed several optimization methods for candidate list maintenance to speed up the search. One of their suggested approaches is to periodically remove irrelevant candidates from the candidate list, in a similar manner to \alg{}. 
%In contrast, \alg{} handles this bottleneck by dedicating a specific cleaning thread that keeps removing irrelevant candidates during the search process. 

Yuan et al.~\cite{yuan:2012} observed that the number of accesses to the sorted lists by NRA could be further reduced by selectively performing the sorted accesses to the different lists (instead of in parallel). They proposed a selection policy that prioritizes the accesses to the sorted lists and cuts down unnecessary accesses. They showed significant cutoff in the number of accesses with respect to the original NRA. However, 
%this algorithm is not safe. Moreover, 
as the authors pointed out, the effectiveness of this approach in terms of run-time latency still has to be explored.

In the IR setting, TA has received much less attention. A few exceptional examples are
\cite{Theobald:2004,Bast:2006,Theobald:2008}, which experimented with TA on web data  using standard IR metrics. Bast et al.~\cite{Bast:2006} optimized the TA scheduling method based on a cost model for sequential and random accesses. Theobald et al.~\cite{Theobald:2008} extended TA for XML query languages. Another work by Theobald et al.~\cite{Theobald:2004} introduced an approximate TA algorithm based on probabilistic arguments: When scanning the posting lists in descending order of local scores, various forms of derived bounds are employed to predict when it is safe, with high probability, to skip candidate items hence trading off accuracy for sorted access. Applying similar probabilistic pruning rules for Sparta  may prove beneficial and is left for future work.


%Gong et al. \cite{Gong:2010} proposed a distributed version of the TA algorithm. It partitions the original inverted files into sub-files . Each process retrieves the top-k results in its portion independently using TA algorithm, and the results from all portions are then merged to provide the final top-k answers. 
 

 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 