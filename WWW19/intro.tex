\section{Introduction}
\label{sec:intro}

% Problem: long queries
% The reliance of Internet users on search engines became enormous during the past two decades. 
Internet users are changing the ways in which they interact with search engines. 
While  early days web queries were short (2.4 terms on average~\cite{Spink:2001:SWP:362968.362979}), 
modern search experiences (query suggestion, reformulation, conversational interfaces, etc.) stimulate their users to submit much longer queries. 
For example, more than 5\% of queries submitted via voice search on mobile devices exceed 10 terms~\cite{sigir/Guy16}. 
At the same time, usability studies
(e.g.,~\cite{Arapakis:2014:IRL:2600428.2609627}) show that users are extremely sensitive to end-to-end delays beyond 500 ms, 
and any excess delay beyond 250 ms leads to material degradation in their experience. Maintaining this \emph{service-level agreement} ({\em SLA}) 
is becoming a major challenge as more queries become longer.

 Most search systems today evaluate queries via a two-stage process~\cite{Wang:2011}. 
The first stage is \emph{top-k retrieval}: it 
 roughly matches the top-k documents most relevant to the query (typically, hundreds to thousands) based on some simple relevance scoring function like tf-idf or BM25~\cite{Baeza-Yates:1999:MIR:553876}. The second stage then re-ranks the  documents via some sophisticated (e.g., machine-learned) function, to produce the final result list (typically, a few tens of documents). 
The first stage  sifts through huge volumes of data and dominates the execution time. 
We therefore focus on the latency induced by this stage. 

%\inred{
The retrieval stage is typically executed in a backend  tier on dedicated search nodes, each of which retrieves the top-k results from a local index shard. 
With the advance of storage and memory technologies, the size of index shards increases, thus increasing the load on each search node. 
A promising approach to handle this load is to  parallelize query evaluation on multiprocessor hardware. 
The current trend in server architecture, which favors parallelism over sequential speed, makes this approach particularly attractive. 
We explore this approach in this paper.
%}


% Approximate is the only option
Obtaining the exact top-k matches out of a large corpus is typically too slow to meet stringent SLA requirements. 
Given that the top-k selection is only the first query  processing step, perfect results are often not essential, as later query processing 
stages can be satisfied by approximate results~\cite{Lin:2015}. Based on this observation we focus on \emph{approximate} 
(sometimes called \emph{non-safe}) query evaluation, tuned to achieve a certain high recall (e.g., $97\%$ or more). 
%of the original results).
% that is deemed sufficient for the application at hand. 

%Top-k retrieval algorithms have been  studied in the query processing literature~\cite{Baeza-Yates:1999:MIR:553876,
%Broder:2003:EQE:956863.956944, Ding:2011:FTD:2009916.2010048}. They capitalize on aggressive pruning of the search space, 
%in order to reduce the traversal time. 
State-of-the-art sequential algorithms for approximate top-k  retrieval, 
%which have both exact (safe) and approximate versions, 
e.g., \emph{Block-Max WAND} ({\em BMW})~\cite{Ding:2011}, serve traditional (short) web queries with impressive speed. 
However, these algorithms do not scale well to verbose queries
\bigdataset{
and big data collections 
}
(as pointed out in~\cite{Bortnikov:2017} and confirmed by our experiments). 
%
Parallelizing these algorithms can improve performance by 30\%--40\%~\cite{rojas2013efficient},    but 
%Rojas et al.~\cite{rojas2013efficient} propose a \emph{parallel} variant of BMW, which we call \emph{pBMW}, achieving 
%performance improvements of 30\%--40\%. 
% ;  we refer to their  algorithm as \emph{pBMW}. %-- with a considerable performance improvement.
our experiments show that such a parallel BMW implementation still has limited scalability, both in the number of query terms and in the index size,
falling short of expected SLAs on long queries or large indices. 
%(e.g., 1 second for 
%12-term query over ClueWeb with eight threads, in our experiments). 
%The algorithm does not achieve high recall when stopped early. 

We present \emph{\alg} (Scalable PARallel Threshold Algorithm) -- a novel 
concurrent algorithm that substantially improves search time for verbose queries over large search indices. Its
%the tail
 query latencies fit well within  real-time SLAs on standard  server hardware. 

\alg's design is inspired by the seminal \emph{Threshold Algorithm (TA)} by Fagin et al.~\cite{Fagin:2003} for retrieving the top-k objects from a database based on an aggregation of attributes that may reside in multiple nodes. 
%%%%Perhaps surprisingly, this algorithm was largely ignored in the web search literature.
%, despite its relevance. 
In the web setting, term posting lists represent attributes and the ranking function is a linear aggregation of term scores. TA has two variants~\cite{Fagin:2003}: (1) \emph{Random Access} ({\em RA}), which  
 relies on random access to  posting lists; 
%\inred{Gali: shouldn't we mention that TA's "regular" posting lists are sorted by term scores?} 
and (2) \emph{No Random Access} ({\em NRA}), which 
 only traverses them sequentially. The former is ineffective in the context of real-time search 
because it requires (1)  a secondary random-access index (which doubles the required index space), 
and (2) random I/O to the on-disk secondary index, which incurs high overhead when the index  is too big to be 
kept entirely in RAM. We therefore base our algorithm on  NRA. 



Transforming NRA into an efficient concurrent algorithm is challenging because coordination around 
shared state can become a major bottleneck unless carefully designed. On the one hand,
sharing state among threads is essential in order to benefit from TA's early stopping feature.
Indeed, our experiments show  that a shared-nothing (partitioned) parallelization of NRA performs two times 
worse than a single-threaded implementation. On the other hand, 
a na\"ive attempt to parallelize NRA using shared memory also results in 
worse performance than the sequential algorithm. \alg\ judiciously shares 
information among threads while significantly reducing the synchronization 
overhead and memory overlap among them, 
%which results in efficient use of hardware resources, 
leading to major performance gains. 

Our results show that \alg\ scales well with 
\bigdataset{
both  corpus size and
}
query length.
E.g., on the 50M-document TREC ClueWeb09B dataset~\cite{ClueWeb09}, %\footnote{\url{https://lemurproject.org/clueweb09}}, 
\alg\ can serve verbose 12-term queries within less than 200 ms, 
whereas a parallel execution of RA processes them in 400 ms, and a parallel execution of BMW takes more than 600 ms. 
%Moreover, \alg\ scales much better with the corpus size:  
\bigdataset{
On a 500M-document index,  its latency is virtually unchanged, whereas 
 \pRA\ requires 2 seconds and pBMW's latency surges to almost 10 seconds.  
  The latter occurs because unlike \alg\ and \pRA, 
pBMW's processing time grows linearly with the index size.
}
%While \alg's advantages are most pronounced for long queries, its 
\alg's throughput on a production-grade query mix (with the query length distribution measured in~\cite{sigir/Guy16}) is twice that of parallel BMW.
\bigdataset{
 on the small corpus, and 25x faster on the large one.
}

Summing up, we present a practical scalable parallel algorithm that significantly improves the state-of-the-art latency  of long query processing.
\bigdataset{
on large search indices.
}
%
The paper proceeds as follows: Section~\ref{sec:problem} defines the top-k retrieval problem and associated metrics. 
Section~\ref{sec:background} gives background on existing algorithms. 
Section~\ref{sec:alg} presents \alg, and Section~\ref{sec:eval}  evaluates it.
Finally, 
Section~\ref{sec:related} discusses related work and Section~\ref{sec:conclusions} concludes the paper.