\section{Related Work}
\label{sec:related}

Verbose queries challenge standard top-k processing techniques in terms of runtime latency. Huston and Croft \cite{Huston:2010} evaluated several sequential query processing techniques for verbose queries, concluding that the most effective one is to simply reduce the length of the query by omitting stop-words or ``stop-structure'' expressions. 
In this work we ignore the query pre-processing phase and consider the query as a bag of words given after textual analysis.

Crane et al.~\cite{Crane:2017} showed that  algorithms that traverse documents in order of id 
are susceptible to tail queries that may take orders of magnitude longer than the median query; approximate query evaluation in WAND and BMW does not significantly reduce the variance. Moreover, in agreement with our findings, they showed that algorithms that access  posting lists in decreasing score order are less sensitive to tail queries due to their effective early termination capability. 

%With the rapid development in multiprocessor hardware, a new line of research has emerged on exploiting multi-threaded programing for top-k retrieval. 
Some previous works,  e.g.,~\cite{Tatikonda:2011,Liu:2018:GUC:3178487.3178512}, studied parallel computation  of conjunctive queries  via posting list interection. 
Note that this problem is different from (and easier than) the problem considered in this paper, where a top-scored document does not necessarily include all query terms. 

Other works~\cite{Bonacic:2010,rojas2013efficient} have parallelized state-of-the-art sequential algorithms like WAND and BMW  by sharding the document space, computing the top-k in each shard independently, and finally merging the results.  Implementations differ in whether threads share a common  
heap and  threshold $\Theta$ or not. A global threshold is  tighter than the threads' local thresholds, hence  less work is done by each of the threads as  more documents can be safely skipped. 
On the other hand, additional overhead is induced by the synchronization (e.g., using locks) needed to guarantee exclusive updates of the shared heap. 
Experimental results~\cite{rojas2013efficient} have shown the superiority of the local-heap approach. The pBMW implementation used in our experiments follows this approach,
but periodically shares the  $\Theta$ values among the threads for improved performance.

Jeon et al.~\cite{Jeon:2014} 
%argued that parallelizing the processing of an individual query gives limited benefits compared to sequential execution since short-running queries, which dominate the workload, do not benefit from parallelization. On the other hand, parallelization substantially reduces the execution time of long-running queries.  They 
presented an adaptive resource management algorithm that chooses the degree of parallelism at runtime for each query, based on predicting high-latency queries.
Such efforts are orthogonal to the performance improvement we achieve via parallelization of (long) queries.
Other works~\cite{Ao:2011,Liu:2018:GUC:3178487.3178512} have explored using GPU hardware for information retrieval;~\cite{Liu:2018:GUC:3178487.3178512} 
focused on adaptively choosing whether to use a CPU or a GPU based on the query's difficulty, and~\cite{Ao:2011} focused on optimizing throughput rather than latency. 
In contrast, our work leverages standard server-grade multi-threaded CPUs. 

The Threshold Algorithm and its variants \cite{Fagin:2001,Fagin:2003,Akbarinia:2007} have been extensively studied by the database community, and have been applied in many relational database systems (for a comprehensive survey see \cite{ilyas2008survey}). 
Mamoulis et al.~\cite{Mamoulis:2007} observed two main phases during NRA processing -- the ``growing phase'', where the candidate list grows, and the ``shrinking phase'' where no new documents can end up in the top-k results, after the first stopping condition is met. They used different data structures for the two phases in order to minimize the number of accesses and the memory requirements. 
Gursky et al.~\cite{Gursky:2008} also noticed the bottleneck in NRA computation derived from NRA's needs to maintain an extremely large number of partially scored candidates. They proposed several optimization methods for candidate list maintenance to speed up the search. One of their suggested approaches is to periodically remove irrelevant candidates from the candidate list, which we also do in \alg.
%In contrast, \alg{} handles this bottleneck by dedicating a specific cleaning thread that keeps removing irrelevant candidates during the search process. 

Yuan et al.~\cite{yuan:2012} observed that the number of accesses to the sorted lists by NRA could be further reduced by selectively performing the sorted accesses to the different lists (instead of in parallel). They proposed a selection policy that prioritizes the accesses to the sorted lists and cuts down unnecessary accesses. They showed significant cutoff in the number of accesses with respect to the original NRA. However, 
%this algorithm is not safe. Moreover, 
as the authors pointed out, the effectiveness of this approach in terms of run-time latency still has to be explored.

In the IR setting, TA has received much less attention. A few exceptional examples are
\cite{Theobald:2004,Bast:2006,Theobald:2008}, which experimented with TA on web data  using standard IR metrics. Bast et al.~\cite{Bast:2006} optimized the TA scheduling method based on a cost model for sequential and random accesses. Theobald et al.~\cite{Theobald:2008} extended TA for XML query languages. Another work by Theobald et al.~\cite{Theobald:2004} introduced an approximate TA algorithm based on probabilistic arguments: When scanning the posting lists in descending order of local scores, various forms of derived bounds are employed to predict when it is safe, with high probability, to skip candidate items hence trading off accuracy for sorted access. Applying similar probabilistic pruning rules for Sparta  may prove beneficial and is left for future work.


%Gong et al. \cite{Gong:2010} proposed a distributed version of the TA algorithm. It partitions the original inverted files into sub-files . Each process retrieves the top-k results in its portion independently using TA algorithm, and the results from all portions are then merged to provide the final top-k answers. 
 

 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 